{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/schwaaweb/aimlds1_11-NLP/blob/master/W11_CCS_AG_NLP_Coding_Challenge__3_Live_Coding_Solution.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qd1ltfV8QShV"
   },
   "source": [
    "### Coding Challenge #3: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iWWDecybQ1zz"
   },
   "source": [
    "In this Coding Challenge, you will cover **Word2vec **which is a popular algorithm for building vector representations of words (i.e. word embeddings). The concept behind Word2Vec is quite straightforward - an assumption is made that the meaning of a word can be inferred by the *context it appears in* or *the company it keeps*. This is similar to stating: “tell me about your friends, and I will tell who you are”. \n",
    "\n",
    "If **2 **words  have very similar neighbors (meaning: the context in which it is used is similar), then the words are most likely quite similar.\n",
    "\n",
    "In this Coding Challenge, you will go through the process of training a Word2vec model with a sample set of documents and then examine certain attributes of the model. After that, you will train a Word2vec model with a large corpus of text and then ascertain the similarity among words in the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "2U6ACjkamo-G",
    "outputId": "e85f3a0f-b77b-4bb0-cf1c-c3e2f535f67c"
   },
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/install.html\n",
    "#!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4890
    },
    "colab_type": "code",
    "id": "6LHb47usm2_L",
    "outputId": "787e102d-ae28-4972-bf86-4eec9da5ea3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/darwinm/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n",
      "CPU times: user 3.65 s, sys: 1.82 s, total: 5.47 s\n",
      "Wall time: 40 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "flAQZT_ZgEEk"
   },
   "source": [
    "**Step #1: ** Tokenize the sample set of documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "IoEKZI3SMJZM",
    "outputId": "034b9eff-0a40-44a1-b40b-d73125aaca53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'dog', 'ran', 'up', 'the', 'steps', 'and', 'entered', 'the', 'owner', \"'s\", 'room', 'to', 'check', 'if', 'the', 'owner', 'was', 'in', 'the', 'room', '.'], ['My', 'name', 'is', 'Thomson', 'Comer', ',', 'commander', 'of', 'the', 'Machine', 'Learning', 'program', 'at', 'Lambda', 'school', '.'], ['I', 'am', 'creating', 'the', 'curriculum', 'for', 'the', 'Machine', 'Learning', 'program', 'and', 'will', 'be', 'teaching', 'the', 'full-time', 'Machine', 'Learning', 'program', '.'], ['Machine', 'Learning', 'is', 'one', 'of', 'my', 'favorite', 'subjects', '.'], ['I', 'am', 'excited', 'about', 'taking', 'the', 'Machine', 'Learning', 'class', 'at', 'the', 'Lambda', 'school', 'starting', 'in', 'April', '.'], ['When', 'does', 'the', 'Machine', 'Learning', 'program', 'kick-off', 'at', 'Lambda', 'school', '?'], ['The', 'batter', 'hit', 'the', 'ball', 'out', 'off', 'AT', '&', 'T', 'park', 'into', 'the', 'pacific', 'ocean', '.'], ['The', 'pitcher', 'threw', 'the', 'ball', 'into', 'the', 'dug-out', '.']]\n",
      "CPU times: user 798 ms, sys: 216 ms, total: 1.01 s\n",
      "Wall time: 7.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Step 1\n",
    "\n",
    "import gensim\n",
    "\n",
    "raw_content = ['The dog ran up the steps and entered the owner\\'s room to check if the owner was in the room.',\n",
    "             'My name is Thomson Comer, commander of the Machine Learning program at Lambda school.',\n",
    "             'I am creating the curriculum for the Machine Learning program and will be teaching the full-time Machine Learning program.',\n",
    "            'Machine Learning is one of my favorite subjects.',\n",
    "            'I am excited about taking the Machine Learning class at the Lambda school starting in April.',\n",
    "                'When does the Machine Learning program kick-off at Lambda school?',\n",
    "                'The batter hit the ball out off AT&T park into the pacific ocean.',\n",
    "                'The pitcher threw the ball into the dug-out.']\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentences = [word_tokenize(text) for text in raw_content]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aews-ibejOF7"
   },
   "source": [
    "**Step #2: ** Train the Word2vec model with tokenized content; size of the word vectors is 5; the word should show-up at least once in the raw content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "657QHc_Rnw1J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 ms, sys: 3.43 ms, total: 16 ms\n",
      "Wall time: 55.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Step 2\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences, min_count=1, size=5)\n",
    "dir(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wor0XODRoc0M"
   },
   "source": [
    "**Step #3: **Output the number of words as well as the list of words in the model's vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XbP2v2erouxO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=69, size=5, alpha=0.025)\n",
      "['The', 'dog', 'ran', 'up', 'the', 'steps', 'and', 'entered', 'owner', \"'s\", 'room', 'to', 'check', 'if', 'was', 'in', '.', 'My', 'name', 'is', 'Thomson', 'Comer', ',', 'commander', 'of', 'Machine', 'Learning', 'program', 'at', 'Lambda', 'school', 'I', 'am', 'creating', 'curriculum', 'for', 'will', 'be', 'teaching', 'full-time', 'one', 'my', 'favorite', 'subjects', 'excited', 'about', 'taking', 'class', 'starting', 'April', 'When', 'does', 'kick-off', '?', 'batter', 'hit', 'ball', 'out', 'off', 'AT', '&', 'T', 'park', 'into', 'pacific', 'ocean', 'pitcher', 'threw', 'dug-out']\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "# Step 3\n",
    "print(model)\n",
    "print(list(model.wv.vocab))\n",
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q0uQwqdLpcpk"
   },
   "source": [
    "**Step #4: **Output the vector of words for the following tokens: **a)** curriculum, **b)** ocean, and **c) **pitcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v86ElbNTp4TY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0429332  -0.0387117  -0.0377904   0.07507995  0.03391911]\n",
      " [ 0.06513041 -0.09689079 -0.09367523 -0.0006524  -0.0025307 ]\n",
      " [ 0.00753346 -0.02386155 -0.0763839  -0.06631807  0.00179267]]\n"
     ]
    }
   ],
   "source": [
    "# Step 4\n",
    "print(model.wv['curriculum', 'ocean', 'pitcher'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzwJ_9R2q0qH"
   },
   "source": [
    "**Step #5:** Now we are going to train the model with more data - larger corpus i.e. the 20 newsgroups text dataset. Fetch the data from the training subset\n",
    "\n",
    "*Reference*: http://scikit-learn.org/stable/datasets/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "2ZNmfKhit91S",
    "outputId": "ca7a57ee-2a7f-42de-8e62-71ba2342e25b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 354 ms, sys: 148 ms, total: 502 ms\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Step 5\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "text_from_corpus = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YmnUlfUMufpb"
   },
   "source": [
    "**Step #6:** Output the metadata for the data that is fetched (investigate the object and what you can do with it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wytJ9wL7u-Zu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'description', 'filenames', 'target', 'target_names']\n",
      "the 20 newsgroups by date dataset\n"
     ]
    }
   ],
   "source": [
    "# Step 6\n",
    "print(dir(text_from_corpus))\n",
    "print(text_from_corpus.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yb54jbjPwIxO"
   },
   "source": [
    "**Step #7: ** Output the # of posts across the different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__SJ7enmwQtC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7\n",
    "len(text_from_corpus.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ukQLf2PXwcwd"
   },
   "source": [
    "**Step #8**: Tokenize the body of text for each post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rj_dddBw5_6"
   },
   "outputs": [],
   "source": [
    "# Step 8\n",
    "import string\n",
    "\n",
    "def process_text(text):\n",
    "  \"\"\"Remove punctuation, lowercase, and tokenize text.\"\"\"\n",
    "  # TODO: check for special cases like \"I'll\"\n",
    "  text = \"\".join([char.lower() for char in text\n",
    "                  if char not in string.punctuation])\n",
    "  return word_tokenize(text)\n",
    "\n",
    "sentences = [process_text(document) for document in text_from_corpus.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMRgRpvJxN0h"
   },
   "source": [
    "**Step #9**: Train the Word2vec model - words should show up at least 3 times in the corpus of text\n",
    "and the size of each word vector is 200 (i.e. dimension = 200)\n",
    "\n",
    "Reference\" Scroll down to the section \"A closer look at the parameter settings\" to review the parameters that can be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PInKDc0vxXr9"
   },
   "outputs": [],
   "source": [
    "# Step 9\n",
    "news_model = Word2Vec(sentences, min_count=3, size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cIg7JCdcxrCr"
   },
   "source": [
    "**Step #10**:  List the number of words in the model's vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "iYEmDcwoxxCB",
    "outputId": "c04d720a-a2e0-43c0-9709-309d5df45bf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43312\n"
     ]
    }
   ],
   "source": [
    "# Step 10\n",
    "print(len(news_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xF-RZy-1CRH2"
   },
   "source": [
    "**Step #11:** Examine word similarity to the word \"Christ\" (find other words most similar to it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hoYABV8E5FgI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jesus', 0.914254367351532),\n",
       " ('lord', 0.8938565850257874),\n",
       " ('spirit', 0.8803279399871826),\n",
       " ('father', 0.8511791825294495),\n",
       " ('son', 0.8467549681663513),\n",
       " ('disciples', 0.8440340161323547),\n",
       " ('satan', 0.8411128520965576),\n",
       " ('holy', 0.8389605283737183),\n",
       " ('messiah', 0.8368544578552246),\n",
       " ('himself', 0.8333228230476379)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 11\n",
    "news_model.wv.most_similar('christ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IPOj0D8PDCBh"
   },
   "source": [
    "**Step #12**: Examine document similarity with Doc2vec to any body of text of your choice\n",
    "\n",
    "*Reference*: https://radimrehurek.com/gensim/models/doc2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D1QS5OoVEKjD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 27s, sys: 3.27 s, total: 2min 31s\n",
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Step 12\n",
    "\n",
    "# We need to train a doc2vec model with the 20 Newsgroup dataset\n",
    "# One of arguments to the model is a \"TaggedDocument\" so we will first go ahead and create a Tagged document\n",
    "\n",
    "# Import TaggedDocument\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# Tokenize each of the posts within the newsgroups\n",
    "sentences = [process_text(document) for document in text_from_corpus.data]\n",
    "\n",
    "# Create a list of tagged_documents\n",
    "# Every item within the tagged_documents list is a tokenized version of the posts \n",
    "tagged_documents_list = []\n",
    "for i, sent in enumerate(sentences):\n",
    "    tagged_documents_list.append(TaggedDocument(sent, [\"sent_{}\".format(i)]))\n",
    "\n",
    "# Examine the first item within tagged_document_lists\n",
    "# print(tagged_documents_list[0])\n",
    "\n",
    "# Train the model with the list of Tagged Documents\n",
    "# size of the vector is 300\n",
    "doc2vec_model = gensim.models.doc2vec.Doc2Vec(tagged_documents_list,\n",
    "                                              vector_size=300)\n",
    "\n",
    "# Get the vector representation for a new document\n",
    "vec_representation = doc2vec_model.infer_vector('I love test driving luxury cars.'.split())\n",
    "#print(vec_representation)\n",
    "\n",
    "#Determine the documents (posts) similar to the new document/post\n",
    "doc2vec_model.docvecs.most_similar([vec_representation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "hrJvs1E0Gg8Y",
    "outputId": "8fca5942-5b07-48ab-899b-a34f16e5c92d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['nntppostinghost', 'surtifiuiono', 'from', 'thomas', 'parsli', 'thomaspifiuiono', 'subject', 're', 'my', 'gun', 'is', 'like', 'my', 'american', 'express', 'card', 'inreplyto', 'vikingiastateedu', 'dan', 'sorensons', 'message', 'of', 'mon', '19', 'apr', '1993', '085242', 'gmt', 'organization', 'dept', 'of', 'informatics', 'university', 'of', 'oslo', 'norway', '1qjmnuinnlmdclemhandheldcom', 'cmm0902734911642thomaspsurtifiuiono', 'viking734945095ponderouscciastateedu', 'cmm0902735132009thomaspsurtifiuiono', 'viking735209562ponderouscciastateedu', 'lines', '51', 'originator', 'thomaspsurtifiuiono', 'i', 'dont', 'remember', 'the', 'figures', 'exactly', 'but', 'there', 'were', 'about', '3500', 'deaths', 'in', 'texas', 'in', '1991', 'that', 'was', 'caused', 'by', 'guns', 'this', 'is', 'more', 'than', 'those', 'beeing', 'killed', 'in', 'caraccidents', 'yes', 'there', 'could', 'be', 'that', 'low', 'sentences', 'or', 'high', 'poverty', 'could', 'influence', 'the', 'figures', 'but', 'theyre', 'still', 'pretty', 'high', 'right', 'i', 'also', 'believe', 'texas', 'has', 'some', 'of', 'the', 'most', 'liberal', 'gunlaws', 'in', 'usa', 'i', 'should', 'not', 'suffer', 'because', 'of', 'others', 'we', 'all', 'agree', 'on', 'this', 'one', 'but', 'we', 'also', 'live', 'in', 'a', 'sociaty', 'and', 'therefor', 'well', 'have', 'to', 'give', 'up', 'some', 'of', 'our', 'freedom', 'note', 'the', 'do', 'you', 'have', 'an', 'insurance', 'then', 'youll', 'have', 'to', 'pay', 'because', 'of', 'what', 'others', 'do', 'do', 'you', 'buy', 'anything', 'you', 'are', 'paying', 'for', 'those', 'who', 'return', 'goods', 'steal', 'or', 'even', 'those', 'who', 'gets', 'a', 'bonus', 'do', 'you', 'live', 'with', 'other', 'people', 'then', 'you', 'cant', 'do', 'ererything', 'youd', 'want', 'burpingfarting', 'playing', 'music', 'loud', 'what', 'the', 'hell', 'is', 'he', 'trying', 'to', 'say', 'when', 'you', 'live', 'in', 'a', 'society', 'usa', 'are', 'stilll', 'counted', 'as', 'one', 'you', 'have', 'to', 'saccrifice', 'the', 'question', 'is', 'how', 'much', 'one', 'state', 'dont', 'remember', 'which', 'texas', 'tried', 'to', 'impose', 'a', 'rule', 'that', 'you', 'could', 'only', 'buy', 'one', 'gun', 'each', 'month', 'think', 'you', 'all', 'know', 'what', 'happened', 'i', 'respect', 'the', 'right', 'to', 'defend', 'yourself', 'but', 'that', 'right', 'should', 'not', 'inflict', 'on', 'other', 'people', 'it', 'seems', 'like', 'you', 'all', 'realize', 'that', 'you', 'have', 'a', 'problem', 'in', 'america', 'the', 'only', 'problem', 'is', 'that', 'you', 'wont', 'take', 'the', 'car', 'away', 'from', 'the', 'drunk', 'driver', 'you', 'hope', 'to', 'cure', 'him', 'first', 'hope', 'life', 'comfirms', 'to', 'the', 'standard', 'of', 'winnie', 'the', 'poh', 'this', 'is', 'not', 'a', 'signature', 'its', 'merely', 'a', 'computergenerated', 'text', 'to', 'waste', 'bandwith', 'and', 'to', 'bring', 'down', 'the', 'evil', 'internet', 'thomas', 'parsli', 'thomaspifiuiono'], ['sent_8027'])\n"
     ]
    }
   ],
   "source": [
    "# Examine the first document in the list above to gauge the similarity\n",
    "print(tagged_documents_list[8027])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RtiIzORkpTes"
   },
   "source": [
    "**Stretch Goal: **\n",
    "\n",
    "Download the pre-trained word vectors from Google. Access the pre-trained vectors via the following link: https://code.google.com/archive/p/word2vec\n",
    "\n",
    "Load the pre-trained word vectors and train the **Word2vec** model\n",
    "\n",
    "Examine the first 100 keys or words of the vocabulary\n",
    "\n",
    "Outputs the vector representation for a select set of words - the words can be of your choice\n",
    "\n",
    "Examine the similarity between words - the words can be of your choice\n",
    "\n",
    "For example: \n",
    "\n",
    "model.similarity('house', 'bungalow')\n",
    "\n",
    "model.similarity('house', 'umbrella')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "W11_CCS--AG--NLP_Coding_Challenge_#3_Live-Coding_Solution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
