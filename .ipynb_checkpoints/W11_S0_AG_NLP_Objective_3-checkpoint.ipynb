{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W11_S0--AG--NLP_Objective_3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/schwaaweb/aimlds1_11-NLP/blob/master/W11_S0_AG_NLP_Objective_3.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "6dg-p_h_3y7M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Objective 3: Comparing documents or words"
      ]
    },
    {
      "metadata": {
        "id": "cQg_ZA1v53wa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Leverage Word Vectors (Word2vec)  to compare words**\n",
        "\n",
        "A key challenge in Natural Language Processing is understanding the language. Text or speech is our main communication channel but the computer can only understand numbers. Therefore the words need to be converted to numbers. The process of converting words to numbers (i.e. vectors) is called Word Embedding. Word2vec is a popular algorithm for building vector representations of words (i.e. word embeddings).  \n",
        "\n",
        "The vector for each word is a semantic depiction of how that word is used in context. 2 words are closer in meaning if they share contexts (Reference: Distribution Hypothesis Theory  -https://en.wikipedia.org/wiki/Distributional_semantics). The Distribution Hypothesis Theory states the following: \n",
        "\n",
        "Linguistic items with similar distributions or similar context have similar meanings.\n",
        "\n",
        "Here is a simple example to clarify the underlying principle of the Distribution Hypothesis Theory:\n",
        "\n",
        "**Sentence 1**: Traffic was light today\n",
        "\n",
        "**Sentence 2**: Traffic was heavy yesterday\n",
        "\n",
        "**Sentence 3**: Prediction is that traffic will be smooth-flowing tomorrow since it is a national holiday\n",
        "\n",
        "Per the Distribution Hypothesis Theory, the words light, heavy, and smooth-flowing must be related in some way since they occur in a similar context (close to the word traffic). Similarly, the words today, yesterday and tomorrow are related since they appear next to a word indicating the state of traffic. Therefore, a word's meaning is governed by the context it occurs in.\n",
        "\n",
        "Another example that highlights the principles that underpin the Distribution Hypothesis Theory is as follows:\n",
        "\n",
        "Let’s take the word ‘mobile’. One of many meanings of this word is the ability to move freely and another one could be a handheld device. In the sentence: “I have become increasingly mobile with the purchase of a car” – we can easily glean that it is the former meaning of mobile. However in the sentence: I just purchased a new mobile phone – mobile takes on the latter meaning (i.e. handheld device)."
      ]
    },
    {
      "metadata": {
        "id": "wuurL6BiA57P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Constructing Vector of Words**"
      ]
    },
    {
      "metadata": {
        "id": "k4PGSRX49lDQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Vector of Words:**\n",
        "\n",
        "![alt text](https://www.dropbox.com/s/ysmrl91jkouh29c/Image%201%20-%20Vector%20of%20Words.JPG?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "eEDclboSF9iF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The table above  illustrates a vector of words where we have a column for every context for every word in a given source text. So for the given text:\n",
        "\n",
        "**\"It was the sunniest of days, it was the rainiest of days\"**, the vector of words is depicted in the above image . \n",
        "\n",
        "The values in each cell highlight how many times the word occurs in the given context. The numbers in the columns constitute that word's vector. For example, the vector for the word \"sunniest\" is:\n",
        "\n",
        "**[0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0]**\n",
        "\n",
        "We have a vector representation that is comprised of 10 dimensions since there are 10 possible contexts (as depicted by the column headers within the table). Once we have a vector representation for the words, we can conduct vector arithmetic (calculate the Euclidean distance between 2 words) to determine which vector representations are similar or closest to each other. In our example scenario, a quick cursory glance of the vector space surfaces that the words \"sunniest\" and \"rainiest\" are similar since they have identical vector representations and the Euclidean distance is 0. \n"
      ]
    },
    {
      "metadata": {
        "id": "LbNL1Mk_MVxf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Word2vec from Google"
      ]
    },
    {
      "metadata": {
        "id": "IIh6v0quI8k4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A common question that arises at this point is how to we handle a large corpus of text given that there will be many possible context to deal with - just dealing with a vector space of 10 dimensions in the example scenario was cumbersome enough. Luckily, instances of pre-trained word vectors are available that can be used to detect word similarities. An instance of a pre-trained word vector from Google is available here:\n",
        "\n",
        "https://code.google.com/archive/p/word2vec/"
      ]
    },
    {
      "metadata": {
        "id": "eJeVmQweMwvN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Other variants of the Word2vec  model\n"
      ]
    },
    {
      "metadata": {
        "id": "jUEPdjnXNTKf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**1) SkipGram:** This variant tries to predict the neighbors of a word.\n",
        "\n",
        "Skip-grams: This variant tries to predict the neighbors’ of a word. In Skip-gram model, we take a center word and a window of context (neighbors) words to train the model and then predict context words out to some window size for each center word.\n",
        "\n",
        "This notion of “context” or “neighboring” words is best described by considering a center word and a window of words around it. \n",
        "\n",
        "For example, if we consider the sentence **“The speedy Proshe drove past the elegant Rolls-Royce”** and a window size of 2, we’d have the following pairs for the skip-gram model:\n",
        "\n",
        "**Text:**\n",
        "The\tspeedy\tProshe\tdrove\tpast\tthe\telegant\tRolls-Royce\n",
        "\n",
        "*Training Sample with window of 2*: (the, speedy), (the, Proshe)\n",
        "\n",
        "**Text:**\n",
        "The\tspeedy\tProshe\tdrove\tpast\tthe\telegant\tRolls-Royce\n",
        "\n",
        "*Training Sample with window of 2*: (speedy, the), (speedy, Proshe), (speedy, drove)\n",
        "\n",
        "**Text:**\n",
        "The\tspeedy\tProshe\tdrove\tpast\tthe\telegant\tRolls-Royce\n",
        "\n",
        "*Training Sample with window of 2*: (Proshe, the), (Proshe, speedy), (Proshe, drove), (Proshe, past)\n",
        "\n",
        "**Text:**\n",
        "The\tspeedy\tProshe\tdrove\tpast\tthe\telegant\tRolls-Royce\n",
        "\n",
        "*Training Sample with window of 2*: (drove, speedy), (drove, Proshe), (drove, past), (drove, the)\n",
        "\n",
        "The **Skip-gram model** is going to output a probability distribution i.e. the probability of a word appearing in context given a center word and we are going to select the vector representation that maximizes the probability.\n"
      ]
    },
    {
      "metadata": {
        "id": "e03CzdpVQENR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://www.dropbox.com/s/c7mwy6dk9k99bgh/Image%202%20-%20SkipGrams.jpg?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "5oZB_g7LN9Kb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**2) Continuous Bag of Words (CBOW):** This model tries to predict a center word based on the neighboring words. In the case of the CBOW model, we input the context words within the window (such as “the”, “Proshe”, “drove”) and aim to predict the target or center word “speedy” (the input to the prediction pipeline is reversed as compared to the SkipGram model).\n",
        "\n",
        "A graphical depiction of the input to output prediction pipeline for both variants of the Word2vec model is attached. The graphical depiction will help crystallize the difference between SkipGrams and Continuous Bag of Words.\n"
      ]
    },
    {
      "metadata": {
        "id": "7eRbeBmqQR9D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://www.dropbox.com/s/k3ddmbtd52wq2li/Image%203%20-%20CBOW%20Model.jpg?raw=1)"
      ]
    }
  ]
}