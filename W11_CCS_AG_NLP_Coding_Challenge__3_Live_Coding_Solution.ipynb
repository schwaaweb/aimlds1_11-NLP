{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W11_CCS--AG--NLP_Coding_Challenge_#3_Live-Coding_Solution.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/schwaaweb/aimlds1_11-NLP/blob/master/W11_CCS_AG_NLP_Coding_Challenge__3_Live_Coding_Solution.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "qd1ltfV8QShV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Coding Challenge #3: Natural Language Processing"
      ]
    },
    {
      "metadata": {
        "id": "iWWDecybQ1zz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this Coding Challenge, you will cover **Word2vec **which is a popular algorithm for building vector representations of words (i.e. word embeddings). The concept behind Word2Vec is quite straightforward - an assumption is made that the meaning of a word can be inferred by the *context it appears in* or *the company it keeps*. This is similar to stating: “tell me about your friends, and I will tell who you are”. \n",
        "\n",
        "If **2 **words  have very similar neighbors (meaning: the context in which it is used is similar), then the words are most likely quite similar.\n",
        "\n",
        "In this Coding Challenge, you will go through the process of training a Word2vec model with a sample set of documents and then examine certain attributes of the model. After that, you will train a Word2vec model with a large corpus of text and then ascertain the similarity among words in the corpus.\n"
      ]
    },
    {
      "metadata": {
        "id": "2U6ACjkamo-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "e85f3a0f-b77b-4bb0-cf1c-c3e2f535f67c"
      },
      "cell_type": "code",
      "source": [
        "# https://radimrehurek.com/gensim/install.html\n",
        "!pip install --upgrade gensim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: gensim in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement not upgraded as not directly required: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.5.7)\n",
            "Requirement not upgraded as not directly required: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement not upgraded as not directly required: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.3)\n",
            "Requirement not upgraded as not directly required: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Requirement not upgraded as not directly required: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.7.37)\n",
            "Requirement not upgraded as not directly required: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
            "Requirement not upgraded as not directly required: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.48.0)\n",
            "Requirement not upgraded as not directly required: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Requirement not upgraded as not directly required: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
            "Requirement not upgraded as not directly required: botocore<1.11.0,>=1.10.37 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.10.37)\n",
            "Requirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
            "Requirement not upgraded as not directly required: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement not upgraded as not directly required: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement not upgraded as not directly required: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.4.16)\n",
            "Requirement not upgraded as not directly required: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.11.0,>=1.10.37->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Requirement not upgraded as not directly required: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.11.0,>=1.10.37->boto3->smart-open>=1.2.1->gensim) (0.14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6LHb47usm2_L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4890
        },
        "outputId": "787e102d-ae28-4972-bf86-4eec9da5ea3f"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /content/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /content/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /content/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /content/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /content/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /content/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /content/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /content/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /content/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /content/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /content/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /content/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /content/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /content/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /content/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /content/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /content/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /content/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /content/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /content/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /content/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /content/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /content/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /content/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /content/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /content/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /content/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /content/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /content/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /content/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /content/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /content/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /content/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /content/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /content/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /content/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /content/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /content/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /content/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /content/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /content/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /content/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /content/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /content/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /content/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /content/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "flAQZT_ZgEEk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #1: ** Tokenize the sample set of documents\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "IoEKZI3SMJZM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "034b9eff-0a40-44a1-b40b-d73125aaca53"
      },
      "cell_type": "code",
      "source": [
        "# Step 1\n",
        "\n",
        "import gensim\n",
        "\n",
        "raw_content = ['The dog ran up the steps and entered the owner\\'s room to check if the owner was in the room.',\n",
        "             'My name is Thomson Comer, commander of the Machine Learning program at Lambda school.',\n",
        "             'I am creating the curriculum for the Machine Learning program and will be teaching the full-time Machine Learning program.',\n",
        "            'Machine Learning is one of my favorite subjects.',\n",
        "            'I am excited about taking the Machine Learning class at the Lambda school starting in April.',\n",
        "                'When does the Machine Learning program kick-off at Lambda school?',\n",
        "                'The batter hit the ball out off AT&T park into the pacific ocean.',\n",
        "                'The pitcher threw the ball into the dug-out.']\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "sentences = [word_tokenize(text) for text in raw_content]\n",
        "print(sentences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['The', 'dog', 'ran', 'up', 'the', 'steps', 'and', 'entered', 'the', 'owner', \"'s\", 'room', 'to', 'check', 'if', 'the', 'owner', 'was', 'in', 'the', 'room', '.'], ['My', 'name', 'is', 'Thomson', 'Comer', ',', 'commander', 'of', 'the', 'Machine', 'Learning', 'program', 'at', 'Lambda', 'school', '.'], ['I', 'am', 'creating', 'the', 'curriculum', 'for', 'the', 'Machine', 'Learning', 'program', 'and', 'will', 'be', 'teaching', 'the', 'full-time', 'Machine', 'Learning', 'program', '.'], ['Machine', 'Learning', 'is', 'one', 'of', 'my', 'favorite', 'subjects', '.'], ['I', 'am', 'excited', 'about', 'taking', 'the', 'Machine', 'Learning', 'class', 'at', 'the', 'Lambda', 'school', 'starting', 'in', 'April', '.'], ['When', 'does', 'the', 'Machine', 'Learning', 'program', 'kick-off', 'at', 'Lambda', 'school', '?'], ['The', 'batter', 'hit', 'the', 'ball', 'out', 'off', 'AT', '&', 'T', 'park', 'into', 'the', 'pacific', 'ocean', '.'], ['The', 'pitcher', 'threw', 'the', 'ball', 'into', 'the', 'dug-out', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Aews-ibejOF7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #2: ** Train the Word2vec model with tokenized content; size of the word vectors is 5; the word should show-up at least once in the raw content"
      ]
    },
    {
      "metadata": {
        "id": "657QHc_Rnw1J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Step 2\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "model = Word2Vec(sentences, min_count=1, size=5)\n",
        "dir(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wor0XODRoc0M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #3: **Output the number of words as well as the list of words in the model's vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "XbP2v2erouxO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Step 3\n",
        "print(model)\n",
        "print(list(model.wv.vocab))\n",
        "print(len(model.wv.vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q0uQwqdLpcpk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #4: **Output the vector of words for the following tokens: **a)** curriculum, **b)** ocean, and **c) **pitcher"
      ]
    },
    {
      "metadata": {
        "id": "v86ElbNTp4TY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Step 4\n",
        "print(model.wv['curriculum', 'ocean', 'pitcher'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tzwJ_9R2q0qH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #5:** Now we are going to train the model with more data - larger corpus i.e. the 20 newsgroups text dataset. Fetch the data from the training subset\n",
        "\n",
        "*Reference*: http://scikit-learn.org/stable/datasets/index.html"
      ]
    },
    {
      "metadata": {
        "id": "2ZNmfKhit91S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "ca7a57ee-2a7f-42de-8e62-71ba2342e25b"
      },
      "cell_type": "code",
      "source": [
        "# Step 5\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "text_from_corpus = fetch_20newsgroups(subset='train')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "YmnUlfUMufpb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #6:** Output the metadata for the data that is fetched (investigate the object and what you can do with it)"
      ]
    },
    {
      "metadata": {
        "id": "wytJ9wL7u-Zu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Step 6\n",
        "print(dir(text_from_corpus))\n",
        "print(text_from_corpus.description)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yb54jbjPwIxO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #7: ** Output the # of posts across the different categories"
      ]
    },
    {
      "metadata": {
        "id": "__SJ7enmwQtC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Step 7\n",
        "len(text_from_corpus.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ukQLf2PXwcwd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #8**: Tokenize the body of text for each post"
      ]
    },
    {
      "metadata": {
        "id": "_rj_dddBw5_6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Step 8\n",
        "import string\n",
        "\n",
        "def process_text(text):\n",
        "  \"\"\"Remove punctuation, lowercase, and tokenize text.\"\"\"\n",
        "  # TODO: check for special cases like \"I'll\"\n",
        "  text = \"\".join([char.lower() for char in text\n",
        "                  if char not in string.punctuation])\n",
        "  return word_tokenize(text)\n",
        "\n",
        "sentences = [process_text(document) for document in text_from_corpus.data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AMRgRpvJxN0h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #9**: Train the Word2vec model - words should show up at least 3 times in the corpus of text\n",
        "and the size of each word vector is 200 (i.e. dimension = 200)\n",
        "\n",
        "Reference\" Scroll down to the section \"A closer look at the parameter settings\" to review the parameters that can be set"
      ]
    },
    {
      "metadata": {
        "id": "PInKDc0vxXr9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Step 9\n",
        "news_model = Word2Vec(sentences, min_count=3, size=200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cIg7JCdcxrCr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #10**:  List the number of words in the model's vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "iYEmDcwoxxCB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c04d720a-a2e0-43c0-9709-309d5df45bf6"
      },
      "cell_type": "code",
      "source": [
        "# Step 10\n",
        "print(len(news_model.wv.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "43312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xF-RZy-1CRH2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #11:** Examine word similarity to the word \"Christ\" (find other words most similar to it)"
      ]
    },
    {
      "metadata": {
        "id": "hoYABV8E5FgI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Step 11\n",
        "news_model.wv.most_similar('christ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IPOj0D8PDCBh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #12**: Examine document similarity with Doc2vec to any body of text of your choice\n",
        "\n",
        "*Reference*: https://radimrehurek.com/gensim/models/doc2vec.html"
      ]
    },
    {
      "metadata": {
        "id": "D1QS5OoVEKjD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Step 12\n",
        "\n",
        "# We need to train a doc2vec model with the 20 Newsgroup dataset\n",
        "# One of arguments to the model is a \"TaggedDocument\" so we will first go ahead and create a Tagged document\n",
        "\n",
        "# Import TaggedDocument\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "# Tokenize each of the posts within the newsgroups\n",
        "sentences = [process_text(document) for document in text_from_corpus.data]\n",
        "\n",
        "# Create a list of tagged_documents\n",
        "# Every item within the tagged_documents list is a tokenized version of the posts \n",
        "tagged_documents_list = []\n",
        "for i, sent in enumerate(sentences):\n",
        "    tagged_documents_list.append(TaggedDocument(sent, [\"sent_{}\".format(i)]))\n",
        "\n",
        "# Examine the first item within tagged_document_lists\n",
        "# print(tagged_documents_list[0])\n",
        "\n",
        "# Train the model with the list of Tagged Documents\n",
        "# size of the vector is 300\n",
        "doc2vec_model = gensim.models.doc2vec.Doc2Vec(tagged_documents_list,\n",
        "                                              vector_size=300)\n",
        "\n",
        "# Get the vector representation for a new document\n",
        "vec_representation = doc2vec_model.infer_vector('I love test driving luxury cars.'.split())\n",
        "#print(vec_representation)\n",
        "\n",
        "#Determine the documents (posts) similar to the new document/post\n",
        "doc2vec_model.docvecs.most_similar([vec_representation])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hrJvs1E0Gg8Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "8fca5942-5b07-48ab-899b-a34f16e5c92d"
      },
      "cell_type": "code",
      "source": [
        "# Examine the first document in the list above to gauge the similarity\n",
        "print(tagged_documents_list[8027])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TaggedDocument(['nntppostinghost', 'surtifiuiono', 'from', 'thomas', 'parsli', 'thomaspifiuiono', 'subject', 're', 'my', 'gun', 'is', 'like', 'my', 'american', 'express', 'card', 'inreplyto', 'vikingiastateedu', 'dan', 'sorensons', 'message', 'of', 'mon', '19', 'apr', '1993', '085242', 'gmt', 'organization', 'dept', 'of', 'informatics', 'university', 'of', 'oslo', 'norway', '1qjmnuinnlmdclemhandheldcom', 'cmm0902734911642thomaspsurtifiuiono', 'viking734945095ponderouscciastateedu', 'cmm0902735132009thomaspsurtifiuiono', 'viking735209562ponderouscciastateedu', 'lines', '51', 'originator', 'thomaspsurtifiuiono', 'i', 'dont', 'remember', 'the', 'figures', 'exactly', 'but', 'there', 'were', 'about', '3500', 'deaths', 'in', 'texas', 'in', '1991', 'that', 'was', 'caused', 'by', 'guns', 'this', 'is', 'more', 'than', 'those', 'beeing', 'killed', 'in', 'caraccidents', 'yes', 'there', 'could', 'be', 'that', 'low', 'sentences', 'or', 'high', 'poverty', 'could', 'influence', 'the', 'figures', 'but', 'theyre', 'still', 'pretty', 'high', 'right', 'i', 'also', 'believe', 'texas', 'has', 'some', 'of', 'the', 'most', 'liberal', 'gunlaws', 'in', 'usa', 'i', 'should', 'not', 'suffer', 'because', 'of', 'others', 'we', 'all', 'agree', 'on', 'this', 'one', 'but', 'we', 'also', 'live', 'in', 'a', 'sociaty', 'and', 'therefor', 'well', 'have', 'to', 'give', 'up', 'some', 'of', 'our', 'freedom', 'note', 'the', 'do', 'you', 'have', 'an', 'insurance', 'then', 'youll', 'have', 'to', 'pay', 'because', 'of', 'what', 'others', 'do', 'do', 'you', 'buy', 'anything', 'you', 'are', 'paying', 'for', 'those', 'who', 'return', 'goods', 'steal', 'or', 'even', 'those', 'who', 'gets', 'a', 'bonus', 'do', 'you', 'live', 'with', 'other', 'people', 'then', 'you', 'cant', 'do', 'ererything', 'youd', 'want', 'burpingfarting', 'playing', 'music', 'loud', 'what', 'the', 'hell', 'is', 'he', 'trying', 'to', 'say', 'when', 'you', 'live', 'in', 'a', 'society', 'usa', 'are', 'stilll', 'counted', 'as', 'one', 'you', 'have', 'to', 'saccrifice', 'the', 'question', 'is', 'how', 'much', 'one', 'state', 'dont', 'remember', 'which', 'texas', 'tried', 'to', 'impose', 'a', 'rule', 'that', 'you', 'could', 'only', 'buy', 'one', 'gun', 'each', 'month', 'think', 'you', 'all', 'know', 'what', 'happened', 'i', 'respect', 'the', 'right', 'to', 'defend', 'yourself', 'but', 'that', 'right', 'should', 'not', 'inflict', 'on', 'other', 'people', 'it', 'seems', 'like', 'you', 'all', 'realize', 'that', 'you', 'have', 'a', 'problem', 'in', 'america', 'the', 'only', 'problem', 'is', 'that', 'you', 'wont', 'take', 'the', 'car', 'away', 'from', 'the', 'drunk', 'driver', 'you', 'hope', 'to', 'cure', 'him', 'first', 'hope', 'life', 'comfirms', 'to', 'the', 'standard', 'of', 'winnie', 'the', 'poh', 'this', 'is', 'not', 'a', 'signature', 'its', 'merely', 'a', 'computergenerated', 'text', 'to', 'waste', 'bandwith', 'and', 'to', 'bring', 'down', 'the', 'evil', 'internet', 'thomas', 'parsli', 'thomaspifiuiono'], ['sent_8027'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RtiIzORkpTes",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Stretch Goal: **\n",
        "\n",
        "Download the pre-trained word vectors from Google. Access the pre-trained vectors via the following link: https://code.google.com/archive/p/word2vec\n",
        "\n",
        "Load the pre-trained word vectors and train the **Word2vec** model\n",
        "\n",
        "Examine the first 100 keys or words of the vocabulary\n",
        "\n",
        "Outputs the vector representation for a select set of words - the words can be of your choice\n",
        "\n",
        "Examine the similarity between words - the words can be of your choice\n",
        "\n",
        "For example: \n",
        "\n",
        "model.similarity('house', 'bungalow')\n",
        "\n",
        "model.similarity('house', 'umbrella')\n"
      ]
    }
  ]
}