{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Objective 1 Part 2.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/schwaaweb/aimlds1_11-NLP/blob/master/M11_CCS1--AG--NLP_Objective_1_Part_2.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "A9EkMOsK9tlQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Objective 1 Part 2: Diagnose the structure of text"
      ]
    },
    {
      "metadata": {
        "id": "Nq3zyNyR6jqF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Utilize Dependency Parsing to glean chunks of meaning and relations**\n",
        "\n",
        "POS tagging labels words in a sentence as adjectives, verbs, nouns, etc. as we saw in the tutorial. However at times, there is a need to determine the dependency between the words and understand relations beyond merely tagging the parts of speech.\n",
        "\n",
        "For Dependency Parsing, we will use spacy which is one of the newer NLP libraries in Python.\n",
        "\n",
        "For your reference, the link below provides the list of dependency tokens (scroll down to the Dependency Tokens section):\n",
        "\n",
        "https://stackoverflow.com/questions/40288323/what-do-spacys-part-of-speech-and-dependency-tags-mean"
      ]
    },
    {
      "metadata": {
        "id": "Wz-4oujHy8ec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1317
        },
        "outputId": "c448e9e0-086a-4022-843c-2bb5a09a939e"
      },
      "cell_type": "code",
      "source": [
        "# Reference: https://spacy.io/\n",
        "!pip install -U spacy\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/31/e60f88751e48851b002f78a35221d12300783d5a43d4ef12fbf10cca96c3/spacy-2.0.11.tar.gz (17.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 17.6MB 1.7MB/s \n",
            "\u001b[?25hRequirement not upgraded as not directly required: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.14.3)\n",
            "Collecting murmurhash<0.29,>=0.28 (from spacy)\n",
            "  Downloading https://files.pythonhosted.org/packages/5e/31/c8c1ecafa44db30579c8c457ac7a0f819e8b1dbc3e58308394fff5ff9ba7/murmurhash-0.28.0.tar.gz\n",
            "Collecting cymem<1.32,>=1.30 (from spacy)\n",
            "  Downloading https://files.pythonhosted.org/packages/f8/9e/273fbea507de99166c11cd0cb3fde1ac01b5bc724d9a407a2f927ede91a1/cymem-1.31.2.tar.gz\n",
            "Collecting preshed<2.0.0,>=1.0.0 (from spacy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/ac/7c17b1fd54b60972785b646d37da2826311cca70842c011c4ff84fbe95e0/preshed-1.0.0.tar.gz (89kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 18.3MB/s \n",
            "\u001b[?25hCollecting thinc<6.11.0,>=6.10.1 (from spacy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/fd/e9f36081e6f53699943381858848f3b4d759e0dd03c43b98807dde34c252/thinc-6.10.2.tar.gz (1.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.2MB 17.7MB/s \n",
            "\u001b[?25hCollecting plac<1.0.0,>=0.9.6 (from spacy)\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Collecting pathlib (from spacy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/9b065a76b9af472437a0059f77e8f962fe350438b927cb80184c32f075eb/pathlib-1.0.1.tar.gz (49kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 18.3MB/s \n",
            "\u001b[?25hCollecting ujson>=1.35 (from spacy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\n",
            "\u001b[K    100% |████████████████████████████████| 194kB 22.8MB/s \n",
            "\u001b[?25hCollecting dill<0.3,>=0.2 (from spacy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/a0/19d4d31dee064fc553ae01263b5c55e7fb93daff03a69debbedee647c5a0/dill-0.2.7.1.tar.gz (64kB)\n",
            "\u001b[K    100% |████████████████████████████████| 71kB 15.1MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5 (from spacy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K    100% |████████████████████████████████| 604kB 21.6MB/s \n",
            "\u001b[?25hCollecting wrapt (from thinc<6.11.0,>=6.10.1->spacy)\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/47/66897906448185fcb77fc3c2b1bc20ed0ecca81a0f2f88eda3fc5a34fc3d/wrapt-1.10.11.tar.gz\n",
            "Collecting tqdm<5.0.0,>=4.10.0 (from thinc<6.11.0,>=6.10.1->spacy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/24/6ab1df969db228aed36a648a8959d1027099ce45fad67532b9673d533318/tqdm-4.23.4-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 17.3MB/s \n",
            "\u001b[?25hCollecting cytoolz<0.9,>=0.8 (from thinc<6.11.0,>=6.10.1->spacy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/e6/ccc124714dcc1bd511e64ddafb4d5d20ada2533b92e3173a4cf09e0d0831/cytoolz-0.8.2.tar.gz (386kB)\n",
            "\u001b[K    100% |████████████████████████████████| 389kB 23.5MB/s \n",
            "\u001b[?25hRequirement not upgraded as not directly required: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy) (1.11.0)\n",
            "Requirement not upgraded as not directly required: termcolor in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy) (1.1.0)\n",
            "Collecting msgpack-python (from thinc<6.11.0,>=6.10.1->spacy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/20/6eca772d1a5830336f84aca1d8198e5a3f4715cd1c7fc36d3cc7f7185091/msgpack-python-0.5.6.tar.gz (138kB)\n",
            "\u001b[K    100% |████████████████████████████████| 143kB 25.0MB/s \n",
            "\u001b[?25hCollecting msgpack-numpy==0.4.1 (from thinc<6.11.0,>=6.10.1->spacy)\n",
            "  Downloading https://files.pythonhosted.org/packages/2e/43/393e30e2768b0357541ac95891f96b80ccc4d517e0dd2fa3042fc8926538/msgpack_numpy-0.4.1-py2.py3-none-any.whl\n",
            "Requirement not upgraded as not directly required: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.9,>=0.8->thinc<6.11.0,>=6.10.1->spacy) (0.9.0)\n",
            "Building wheels for collected packages: spacy, murmurhash, cymem, preshed, thinc, pathlib, ujson, dill, regex, wrapt, cytoolz, msgpack-python\n",
            "  Running setup.py bdist_wheel for spacy ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/fb/00/28/75c85d5135e7d9a100639137d1847d41e914ed16c962d467e4\n",
            "  Running setup.py bdist_wheel for murmurhash ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/b8/94/a4/f69f8664cdc1098603df44771b7fec5fd1b3d8364cdd83f512\n",
            "  Running setup.py bdist_wheel for cymem ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/55/8d/4a/f6328252aa2aaec0b1cb906fd96a1566d77f0f67701071ad13\n",
            "  Running setup.py bdist_wheel for preshed ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/8f/85/06/2d132fb649a6bbcab22487e4147880a55b0dd0f4b18fdfd6b5\n",
            "  Running setup.py bdist_wheel for thinc ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/d8/5c/3e/9acf5d9974fb1c9e7b467563ea5429c9325f67306e93147961\n",
            "  Running setup.py bdist_wheel for pathlib ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/f9/b2/4a/68efdfe5093638a9918bd1bb734af625526e849487200aa171\n",
            "  Running setup.py bdist_wheel for ujson ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/28/77/e4/0311145b9c2e2f01470e744855131f9e34d6919687550f87d1\n",
            "  Running setup.py bdist_wheel for dill ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/99/c4/ed/1b64d2d5809e60d5a3685530432f6159d6a9959739facb61f2\n",
            "  Running setup.py bdist_wheel for regex ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "  Running setup.py bdist_wheel for wrapt ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/48/5d/04/22361a593e70d23b1f7746d932802efe1f0e523376a74f321e\n",
            "  Running setup.py bdist_wheel for cytoolz ... \u001b[?25l-"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/f8/b1/86/c92e4d36b690208fff8471711b85eaa6bc6d19860a86199a09\n",
            "  Running setup.py bdist_wheel for msgpack-python ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/d5/de/86/7fa56fda12511be47ea0808f3502bc879df4e63ab168ec0406\n",
            "Successfully built spacy murmurhash cymem preshed thinc pathlib ujson dill regex wrapt cytoolz msgpack-python\n",
            "Installing collected packages: murmurhash, cymem, preshed, wrapt, tqdm, cytoolz, plac, dill, pathlib, msgpack-python, msgpack-numpy, thinc, ujson, regex, spacy\n",
            "Successfully installed cymem-1.31.2 cytoolz-0.8.2 dill-0.2.7.1 msgpack-numpy-0.4.1 msgpack-python-0.5.6 murmurhash-0.28.0 pathlib-1.0.1 plac-0.9.6 preshed-1.0.0 regex-2017.4.5 spacy-2.0.11 thinc-6.10.2 tqdm-4.23.4 ujson-1.35 wrapt-1.10.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K15I1a_tzCOU",
        "colab_type": "code",
        "colab": {},
        "outputId": "497130de-2ce6-45fb-8f1c-de308c03eb27"
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x_S3btsRzYMS",
        "colab_type": "code",
        "colab": {},
        "outputId": "26c843f0-53d0-429e-e498-563b5aad2e11"
      },
      "cell_type": "code",
      "source": [
        "# Import spacy\n",
        "import spacy\n",
        "\n",
        "# Load English version of tokenizer, tagger, parser\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "\n",
        "# Define a function to show what word each word depends upon\n",
        "def show_dependency(rawText):\n",
        "    tokens = nlp(rawText)\n",
        "    print(tokens)\n",
        "    for token in tokens:\n",
        "        print(\"{} : {}\".format(token.orth_,token.dep_))\n",
        "\n",
        "# Call the function\n",
        "show_dependency(\"The batter hit the ball out of AT&T park into the pacific ocean.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jYQLorZC9Zj0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Determine the head of a sentence**\n",
        "\n",
        "The head of a sentence governs the central structure of the sentence. It is the most important word of the sentence and it is usually a verb. In the previous example, the word \"hit\" is the root around which the rest of the sentence revolves. The word \"hit\" has a direct object i.e. the ball and a subject i.e. the batter"
      ]
    },
    {
      "metadata": {
        "id": "r0Omj0ua9emZ",
        "colab_type": "code",
        "colab": {},
        "outputId": "c5358085-d57a-48d8-a6cd-579a5a788584"
      },
      "cell_type": "code",
      "source": [
        "# Import spacy\n",
        "import spacy\n",
        "\n",
        "# Load English version of tokenizer, tagger, parser\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "# Define a function to show what word each word depends upon\n",
        "def show_dependency(rawText):\n",
        "    tokens = nlp(rawText)\n",
        "    for token in tokens:\n",
        "        print(\" {} : {} : {} : {}\".format(\n",
        "            token.orth_, token.pos_, token.dep_, token.head))\n",
        "\n",
        "# Print the header\n",
        "print(\"token : POS : dep. : head\")\n",
        "print(\"#########################\")\n",
        "\n",
        "# Call the function\n",
        "show_dependency(\"The batter hit the ball out of AT&T park into the pacific ocean\")\n",
        "\n",
        "# In the above example, the central structure of the sentence\n",
        "# is dictated by the ROOT which is the word \"hit\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g7_oTxF3-mD2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Find Named Entities in a body of text\n",
        "\n",
        "So far, we have explored part-of-speech tagging, dependency parsing, and identifying the head of a sentence. Now, we will look at finding proper nouns or named entities. Named Entity Recognition (NER) helps understand what a body of text is about; essentially, it provides an insight into what is being referred to in the body of text."
      ]
    },
    {
      "metadata": {
        "id": "vCU17gO8_JiT",
        "colab_type": "code",
        "colab": {},
        "outputId": "17337669-3b3e-4016-cd49-a961dfe78b20"
      },
      "cell_type": "code",
      "source": [
        "# Import spacy\n",
        "# https://spacy.io/usage/spacy-101\n",
        "import spacy\n",
        "\n",
        "# Load English version of tokenizer, tagger, parser\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "# Define a function to show what word each word depends upon\n",
        "def show_dependency(rawText):\n",
        "    entities = nlp(rawText)\n",
        "    for entity in entities.ents:\n",
        "        print(\"{} : {}\".format(entity.text,entity.label_))\n",
        "\n",
        "# Call the function\n",
        "show_dependency(\"Alexander Martino Rodrigues-Schmidt hit the ball out off AT&T park into the pacific ocean. The park is located in San Francisco\")\n",
        "\n",
        "# GPE - stands for Geo-Political Entity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oy1VWB152dXj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Access built-in corpuses**"
      ]
    },
    {
      "metadata": {
        "id": "y6B4IQJhwSJr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GgjfiFxW2l3U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A corpus is a collection of text documents, and a corpora is the plural of corpus. When working with real-world **Natural Language Processing** (NLP) problems, there is a need to work with huge amounts of data. This data is generally available in the form of a corpus externally on the World Wide Web or as an add-on of the NLTK package. For example, to create a spell checker, you need a huge corpus of words to match against. Within this objective, we will explore how to access and work with the built-in corpora and we will walk though the process of creating a custom corpus; a custom corpus is essentially just a bunch of text files in a directory."
      ]
    },
    {
      "metadata": {
        "id": "frnUfnBY3QKe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Load, access and perform operations on a built-in corpus**\n",
        "\n",
        "NLTK provides many corpuses, the complete list of built-in corpora provided within NLTK is available at: http://www.nltk.org/nltk_data. To access the set of data packages included with NLTK, you will have to download the packages fist. Instructions to download the data packages can be accessed via the following link: https://www.nltk.org/data.html"
      ]
    },
    {
      "metadata": {
        "id": "J6KKEl0y2kvJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "80b60de7-963c-420f-9044-0cfe7ecb6f0e"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('all')\n",
        "\n",
        "# Inaugural is one of the data packages included within NLTK\n",
        "\n",
        "# Import the \"inaugural\" data package\n",
        "from nltk.corpus import inaugural\n",
        "\n",
        "# Output the list of all Inaugural addresses included\n",
        "# print(inaugural.fileids())\n",
        "\n",
        "# All words from a subset of the Inaugural Address Corpus that begin with future\n",
        "# or fellow are counted; separate counts are kept for each inaugural address;\n",
        "# these are plotted so that trends in usage over time can be observed\n",
        "condfreqdist = nltk.ConditionalFreqDist(\n",
        "            (tget, fileid[:4])\n",
        "            for fileid in (inaugural.fileids()[0:25])\n",
        "            for w in inaugural.words(fileid)\n",
        "            for tget in ['future', 'fellow']\n",
        "            if w.lower().startswith(tget)) \n",
        "condfreqdist.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mfgmdkBK4oYH",
        "colab_type": "code",
        "colab": {},
        "outputId": "d4a9e8f5-c224-4976-8dc1-6a8b598bfdac"
      },
      "cell_type": "code",
      "source": [
        "# Accessing the \"reuters\" data package\n",
        "# Import the \"reuters\" data package\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "# List the files within the \"reuters\" corpus\n",
        "reuters.fileids()\n",
        "\n",
        "# Fetch the words from a specific\n",
        "\n",
        "words14862 = reuters.words(['test/14862'])\n",
        "print(words14862)\n",
        "\n",
        "# Fetch the first 25 words from the file which is part of the corpus\n",
        "words25 = reuters.words(['test/14862'])[:25]\n",
        "print(words25)\n",
        "\n",
        "# The corpus is a list of files hierarchically categorized into 90 topics\n",
        "# Examine the categories within the Reuters corpus\n",
        "reutersCategories = reuters.categories()\n",
        "print(reutersCategories)\n",
        "\n",
        "# Fetch the files associated with a specific category\n",
        "print(reuters.fileids(categories='barley'))\n",
        "\n",
        "# Fetch the words from the specific category\n",
        "print(reuters.words(categories='barley')[:25])\n",
        "\n",
        "# Fetch words from 2 categories\n",
        "print(reuters.words(categories=['barley', 'carcass']))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}