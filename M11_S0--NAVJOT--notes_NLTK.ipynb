{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notes - NLTK.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/schwaaweb/aimlds1_11-NLP/blob/master/M11_S0--NAVJOT--notes_NLTK.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "PJnhN5L5Hcet",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "2a87a68d-9d19-469e-fc3f-f042941810b6"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 4.2MB/s \n",
            "\u001b[?25hRequirement not upgraded as not directly required: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Running setup.py bdist_wheel for nltk ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3O6KV4RSHwwz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4625
        },
        "outputId": "418ab408-2961-4dd9-ebf2-f304a2c5624e"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data]    | Downloading package names to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package webtext to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     /content/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "rBMfVOMfJLWk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ]
    },
    {
      "metadata": {
        "id": "KJtKSSctH8_S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "852563bb-01b4-4225-a633-835590b60d02"
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import LineTokenizer, SpaceTokenizer, TweetTokenizer\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "\n",
        "text = \"Mr. Jones loves pizzas.\\nHe has a Ph.D. in Pieology.\"\n",
        "\n",
        "# LineTokenizer: breaks body of text into lines\n",
        "lineTokenizer = LineTokenizer()\n",
        "print(\"LineTokenizer:\", lineTokenizer.tokenize(text))\n",
        "print()\n",
        "\n",
        "# SpaceTokenizer: splits the body of text on the space character\n",
        "spaceTokenizer = SpaceTokenizer()\n",
        "print(\"SpaceTokenizer:\", spaceTokenizer.tokenize(text))\n",
        "print()\n",
        "\n",
        "# Sentence Tokenizer: breaks the body of text into sentences\n",
        "print(\"Sentence Tokenizer:\")\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "for sent in sentences:\n",
        "  print(sent)\n",
        "print()\n",
        "\n",
        "# Word Tokenizer: breaks up the words and punctuation marks\n",
        "print(\"Word Tokenizer:\", nltk.word_tokenize(text))\n",
        "print()\n",
        "\n",
        "# Tweet Tokenizer: use this tokenizer when dealing with tweets\n",
        "# as they contain special strings (e.g. #)\n",
        "tweetTokenizer = TweetTokenizer()\n",
        "print(\"TweetTokenizer:\", tweetTokenizer.tokenize(\"This is a awesome #gesture: :-) :-D\"))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LineTokenizer: ['Mr. Jones loves pizzas.', 'He has a Ph.D. in Pieology.']\n",
            "\n",
            "SpaceTokenizer: ['Mr.', 'Jones', 'loves', 'pizzas.\\nHe', 'has', 'a', 'Ph.D.', 'in', 'Pieology.']\n",
            "\n",
            "Sentence Tokenizer:\n",
            "Mr. Jones loves pizzas.\n",
            "He has a Ph.D. in Pieology.\n",
            "\n",
            "Word Tokenizer: ['Mr.', 'Jones', 'loves', 'pizzas', '.', 'He', 'has', 'a', 'Ph.D.', 'in', 'Pieology', '.']\n",
            "\n",
            "TweetTokenizer: ['This', 'is', 'a', 'awesome', '#gesture', ':', ':-)', ':-D']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UcUjmyFaJNAR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Stop words"
      ]
    },
    {
      "metadata": {
        "id": "Q01rrxLxJFJ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "6ed72963-b67a-4eb3-ad05-4d0d10047e67"
      },
      "cell_type": "code",
      "source": [
        "en_stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "raw_text = \"The dog ran up the steps and entered the owner's room to check if the owner was in the room\"\n",
        "tokens = word_tokenize(raw_text)\n",
        "\n",
        "tokens_minus_stop = [token for token in tokens if token.lower() not in en_stopwords]\n",
        "print(\"tokens:\", tokens)\n",
        "print(\"tokens:\", tokens_minus_stop)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokens: ['The', 'dog', 'ran', 'up', 'the', 'steps', 'and', 'entered', 'the', 'owner', \"'s\", 'room', 'to', 'check', 'if', 'the', 'owner', 'was', 'in', 'the', 'room']\n",
            "tokens: ['dog', 'ran', 'steps', 'entered', 'owner', \"'s\", 'room', 'check', 'owner', 'room']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hvbjn1WoMUmI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ]
    },
    {
      "metadata": {
        "id": "_jIECaKNJ_3x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "736de856-1a1d-41e0-f382-5d4e58d06eaf"
      },
      "cell_type": "code",
      "source": [
        "from nltk import PorterStemmer, LancasterStemmer\n",
        "\n",
        "raw_text = \"My name is Thomson Comer, commander-in-chief of the Machine Learning program at Lambda school. I am creating the curriculum for the Machine Learning program and will be teaching the full-time Machine Learning program beginning in April 2018.\"\n",
        "\n",
        "tokens = word_tokenize(raw_text)\n",
        "\n",
        "p_stemmer = PorterStemmer()\n",
        "p_stems = [p_stemmer.stem(t) for t in tokens]\n",
        "print(\"PorterStemmer:\", p_stems)\n",
        "\n",
        "l_stemmer = LancasterStemmer()\n",
        "l_stems = [l_stemmer.stem(t) for t in tokens]\n",
        "print(\"LancasterStemmer:\", l_stems)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PorterStemmer: ['My', 'name', 'is', 'thomson', 'comer', ',', 'commander-in-chief', 'of', 'the', 'machin', 'learn', 'program', 'at', 'lambda', 'school', '.', 'I', 'am', 'creat', 'the', 'curriculum', 'for', 'the', 'machin', 'learn', 'program', 'and', 'will', 'be', 'teach', 'the', 'full-tim', 'machin', 'learn', 'program', 'begin', 'in', 'april', '2018', '.']\n",
            "LancasterStemmer: ['my', 'nam', 'is', 'thomson', 'com', ',', 'commander-in-chief', 'of', 'the', 'machin', 'learn', 'program', 'at', 'lambd', 'school', '.', 'i', 'am', 'cre', 'the', 'curricul', 'for', 'the', 'machin', 'learn', 'program', 'and', 'wil', 'be', 'teach', 'the', 'full-time', 'machin', 'learn', 'program', 'begin', 'in', 'april', '2018', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xkCoMjJQW1dm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### POS Tagging"
      ]
    },
    {
      "metadata": {
        "id": "xO19-_CzW1D0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "53497533-ac16-4a55-a434-68c8c44ee1cf"
      },
      "cell_type": "code",
      "source": [
        "# The POS identifiers for this example is outlined below\n",
        "# NNP -- proper noun\n",
        "# VBZ -- verb, 3rd person sing. present\n",
        "# DT  --  determiner\n",
        "# NN  -- noun, singular \n",
        "# IN  --  preposition/subordinating conjunction\n",
        "# CC --  coordinating conjunction\n",
        "# NNPS -- proper noun, plural\n",
        "\n",
        "sentence = \"Sorry, I'll call later\" #\"Sacramento is the Capital of California and Washington DC is the Capital of the United States\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Sorry', 'NNP'), (',', ','), ('I', 'PRP'), (\"'ll\", 'MD'), ('call', 'VB'), ('later', 'RB')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UFkSmkyXYRH2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, pos_tag\n",
        "import nltk.corpus.reader.wordnet as wordnet\n",
        "\n",
        "def pos_for_WordNetLemmatizer(token):  \n",
        "  word, tag = pos_tag([token])[0]  \n",
        "  \n",
        "  if tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
        "    return wordnet.VERB\n",
        "  elif tag in ['JJ', 'JJR', 'JJS']:\n",
        "    return wordnet.ADJ\n",
        "  elif tag in ['RB', 'RBR', 'RBS']:\n",
        "    return wordnet.ADV\n",
        "  \n",
        "  return wordnet.NOUN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kiHrSE9TNfNB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "metadata": {
        "id": "aU7Sz0rRMvg_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "71fcb86b-4f09-49e9-d5cd-6d562f343c3f"
      },
      "cell_type": "code",
      "source": [
        "from nltk import WordNetLemmatizer\n",
        "\n",
        "raw_text = \"My name is Thomson Comer, commander-in-chief of the Machine Learning program at Lambda school. I am creating the curriculum for the Machine Learning program and will be teaching the full-time Machine Learning program beginning in April 2018. We are driving full steam forward to have enough content ready for the launch.\"\n",
        "\n",
        "tokens = word_tokenize(raw_text)\n",
        "\n",
        "p_stemmer = PorterStemmer()\n",
        "p_stems = [p_stemmer.stem(t) for t in tokens]\n",
        "print(\"PorterStemmer:\", p_stems)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(t, pos_for_WordNetLemmatizer(t)) for t in tokens]\n",
        "print(\"Lemmatizer:\", lemmas)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PorterStemmer: ['My', 'name', 'is', 'thomson', 'comer', ',', 'commander-in-chief', 'of', 'the', 'machin', 'learn', 'program', 'at', 'lambda', 'school', '.', 'I', 'am', 'creat', 'the', 'curriculum', 'for', 'the', 'machin', 'learn', 'program', 'and', 'will', 'be', 'teach', 'the', 'full-tim', 'machin', 'learn', 'program', 'begin', 'in', 'april', '2018', '.', 'We', 'are', 'drive', 'full', 'steam', 'forward', 'to', 'have', 'enough', 'content', 'readi', 'for', 'the', 'launch', '.']\n",
            "Lemmatizer: ['My', 'name', 'be', 'Thomson', 'Comer', ',', 'commander-in-chief', 'of', 'the', 'Machine', 'Learning', 'program', 'at', 'Lambda', 'school', '.', 'I', 'be', 'create', 'the', 'curriculum', 'for', 'the', 'Machine', 'Learning', 'program', 'and', 'will', 'be', 'teach', 'the', 'full-time', 'Machine', 'Learning', 'program', 'begin', 'in', 'April', '2018', '.', 'We', 'be', 'drive', 'full', 'steam', 'forward', 'to', 'have', 'enough', 'content', 'ready', 'for', 'the', 'launch', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}